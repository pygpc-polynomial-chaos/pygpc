{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nIntroduction to uncertainty analysis\n====================================\n\nTypically, we have some model, which depends on parameters, e.g., $\\tau_{e,i}, \\tau_{e,i}, ..., C_i$\nand we are interested in several quantities of interest (QOI), e.g., $y_j$, we can compute, when we\nrun the model with the desired set of parameters.\n\n![](../../../examples/images/Uncertainty_Analysis.png)\n\n    :width: 700\n    :align: center\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uncertainty analysis using Monte Carlo methods\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIn order to determine the statistical information of the output quantities of a system, such as the mean\nor the variance, one of the most common methods is Monte Carlo sampling (MC). The MC method is based on\nrepetitive calculations of the forward model, while defining the random inputs according to their\nprobability distributions. This results in an ensemble of solutions from which statistical properties\ncan be derived. However, a large number of simulations is needed due to the slow convergence rate of the\nMC method. The mean for example converges with $1/\\sqrt{N}$, where $N$ is the number of deterministic\nforward calculations. This limits the applicability to problems with low computational cost.\n\nHence, the whole UQ problem reduces to find the *unknown* transfer function (grey box) from which\nwe can infer all necessary information (statistics, sensitivities, etc)\n\nExample\n^^^^^^^\nThree-dimensional test function of Ishigami.\n\n\\begin{align}y = \\sin(x_1) + a \\sin(x_2)^2 + b x_3^4 \\sin(x_1)\\end{align}\n\nThe Ishigami function of Ishigami & Homma (1990) is used as an example for uncertainty\nand sensitivity analysis methods, because it exhibits strong nonlinearity and nonmonotonicity.\nIt also has a peculiar dependence on $x_3$, as described by Sobol and Levitan (1999).\nThe values of a and b used by Crestaux et al. (2007) and Marrel et al. (2009) are: a = 7 and b = 0.1.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pygpc\nimport time\nimport numpy as np\nfrom IPython import display\nimport matplotlib.pyplot as plt\n\n# Parameters\np = dict()\np[\"x1\"] = (np.random.rand(int(1e5))-0.5)*2*np.pi\np[\"x2\"] = (np.random.rand(int(1e5))-0.5)*2*np.pi\np[\"x3\"] = np.zeros(p[\"x1\"].shape)\np[\"a\"] = 7\np[\"b\"] = 0.1\n\n# Model\nmodel = pygpc.testfunctions.Ishigami().set_parameters(p)\n\n# Run simulations\ny = model.simulate()\n\n# Plot results\nfig = plt.figure(figsize=[15, 4])\n\nN = np.logspace(1, 5, 5).astype(int)\nmean = []\nstd = []\n\nfor i in range(len(N)):\n    ax1 = fig.add_subplot(131, projection='3d')\n    ax1.scatter(p[\"x1\"][:N[i]],\n                p[\"x2\"][:N[i]],\n                y[:N[i]],\n                s=4, c=y[0:N[i]].flatten(), cmap=\"jet\")\n    ax1.set_xlabel(\"x1\")\n    ax1.set_ylabel(\"x2\")\n    ax1.set_zlabel(\"y\")\n    ax1.view_init(elev=45, azim=180)\n\n    ax2 = fig.add_subplot(132)\n    mean.append(np.mean(y[:N[i]]))\n    ax2.plot(N[:i+1], mean)\n    ax2.set_xscale(\"log\")\n    ax2.grid(True)\n    ax2.set_xlabel(\"N\")\n    ax2.set_ylabel(\"Mean\")\n\n    ax3 = fig.add_subplot(133)\n    std.append(np.std(y[:N[i]]))\n    ax3.plot(N[:i+1], std)\n    ax3.set_xscale(\"log\")\n    ax3.grid(True)\n    ax3.set_xlabel(\"N\")\n    ax3.set_ylabel(\"STD\")\n    _ = display.display(plt.gcf())\n\n    if i < (len(N)-1):\n        display.clear_output(wait=True)\n\n    time.sleep(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be clearly seen that the mean and the standard deviation converge slowly with increasing $N$,\ndemonstrating the bad convergence properties of standard Monte Carlo approaches.\n\nReferences\n^^^^^^^^^^\n.. [1] Ishigami, T., Homma, T. (1990, December). An importance quantification\n   technique in uncertainty analysis for computer models. In Uncertainty\n   Modeling and Analysis, 1990. Proceedings., First International Symposium\n   on (pp. 398-403). IEEE.\n\n.. [2] Sobol', I.M., Levitan, Y.L. (1999). On the use of variance reducing\n   multipliers in Monte Carlo computations of a global sensitivity index.\n   Computer Physics Communications, 117(1), 52-61.\n\n.. [3] Crestaux, T., Martinez, J.-M., Le Maitre, O., & Lafitte, O. (2007).\n   Polynomial chaos expansion for uncertainties quantification and sensitivity analysis [PowerPoint slides].\n   Retrieved from SAMO 2007 website: http://samo2007.chem.elte.hu/lectures/Crestaux.pdf.\n\n.. [4] Marrel, A., Iooss, B., Laurent, B., & Roustant, O. (2009).\n   Calculations of sobol indices for the gaussian process metamodel.\n   Reliability Engineering & System Safety, 94(3), 742-751.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}